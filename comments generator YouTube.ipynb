{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import wordpunct_tokenize # our main tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_comments = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102/102 [00:04<00:00, 24.79it/s]\n"
     ]
    }
   ],
   "source": [
    "total_text_values = np.array([])\n",
    "files = os.listdir(path_comments)\n",
    "\n",
    "for filename in tqdm(files, position=0, leave=True):    \n",
    "    comments_values = pd.read_csv(os.path.join(path_comments, filename), usecols=['text'])['text'].to_numpy()\n",
    "    total_text_values = np.hstack((total_text_values, comments_values))\n",
    "    \n",
    "total_text_values = pd.Series(total_text_values, name='text') # Convert to pandas.Series for apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values = total_text_values.sample(300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(string):\n",
    "    \n",
    "    rus = re.compile(\"[–∞-—è–ê-–Ø]+\")\n",
    "    string = string.lower()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values = total_text_values.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values[total_text_values != ''].to_csv(os.path.join(os.getcwd(), 'comments_preprocessed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 1)\n"
     ]
    }
   ],
   "source": [
    "total_text_values = pd.read_csv(os.path.join(os.getcwd(), 'comments_preprocessed.csv'))\n",
    "print(total_text_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPunctEmojiTokenizer:\n",
    "    \n",
    "    def __init__(self, min_word_freq=5, max_word_freq=0.85, parse_emoji=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "        --------------\n",
    "        min_word_freq: float\n",
    "            minimal number of documents wich are contains word\n",
    "            \n",
    "        max_word_freq: float\n",
    "            maximal fraction of documents wich are contains word\n",
    "            \n",
    "        Functions:\n",
    "        --------------\n",
    "        \n",
    "        fit:\n",
    "        word_to_tok:\n",
    "        tok_to_word:\n",
    "        \n",
    "        Notes: \n",
    "            free indexes:0 - PAD token, 1 - SOS token, 2 - EOS token, 3 - UNK token\n",
    "        --------------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        from collections import defaultdict\n",
    "        import numpy as np\n",
    "        \n",
    "        self.total_samples = 0\n",
    "        self.tok_unk = 0\n",
    "        self.punct = np.array(['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
    "                               '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', \n",
    "                               ']', '^', '_', '`', '{', '|', '}', '~'])\n",
    "        \n",
    "        self.min_word_freq = min_word_freq\n",
    "        self.max_word_freq = max_word_freq\n",
    "        self.parse_emoji = parse_emoji\n",
    "                \n",
    "        self.word_total_freq = defaultdict(int) # here we store total word frequency\n",
    "        self.word_sentence_freq = defaultdict(int) # here we store word frequency in each sentence\n",
    "        \n",
    "        self.word_tok_dict = defaultdict(int) # dict to convert word to token\n",
    "        self.tok_word_dict = defaultdict(str) # dict to convert token to word\n",
    "        \n",
    "        \n",
    "    def tokenize_sentence(self, sentence):\n",
    " \n",
    "        tokens_final = []\n",
    "        for token in wordpunct_tokenize(sentence):\n",
    "            \n",
    "            if re.findall('[^\\w\\d\\s]', token): # this part check for stacked punctuation \n",
    "                if np.all(np.isin(np.array(list(token)),  self.punct)):\n",
    "                    tokens_final.extend(list(token))\n",
    "                else:\n",
    "                    tokens_final.append(token)\n",
    "            else:\n",
    "                tokens_final.append(token)\n",
    "\n",
    "        return tokens_final\n",
    "    \n",
    "    \n",
    "    def tokenize_emoji_sentence(self, sentence):\n",
    "        # Tokenize sentence with emojies in correct way\n",
    "        from nltk import wordpunct_tokenize\n",
    "        \n",
    "        tokens = []\n",
    "        for tok in wordpunct_tokenize(sentence):\n",
    "            if emoji.get_emoji_regexp().search(tok):\n",
    "                tokens.extend([i for i in list(tok) if i != ''])\n",
    "                \n",
    "            elif re.findall('[^\\w\\d\\s]', tok):\n",
    "                if np.all(np.isin(np.array(list(tok)),  self.punct)):\n",
    "                    tokens.extend(list(tok))\n",
    "                else:\n",
    "                    tokens.append(tok)\n",
    "            else:\n",
    "                tokens.append(tok)\n",
    "\n",
    "        return tokens\n",
    "     \n",
    "        \n",
    "    def fit(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        corpus: np.ndarray\n",
    "            array of arrays of strings: array(array(str), array(str), ...)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "        from nltk import wordpunct_tokenize # our main tokenizer\n",
    "        from collections import Counter\n",
    "        \n",
    "        \n",
    "        if not isinstance(corpus, np.ndarray):\n",
    "            raise TypeError(f'corpus has to be numpy.ndarray type, got {type(corpus)}')\n",
    "        self.total_samples = corpus.shape[0]\n",
    "        \n",
    "        iter_ = 0\n",
    "        for sentence in tqdm(corpus, position=0, leave=True):\n",
    "            if isinstance(sentence, np.ndarray) or isinstance(sentence, list):\n",
    "                sentence = sentence[0]\n",
    "                \n",
    "            if not isinstance(sentence, str):\n",
    "                raise TypeError(f'Sentence must be string, found {type(sentence)} type on index {iter_}')\n",
    "                        \n",
    "                    \n",
    "            # Emoji dealing with part\n",
    "            if self.parse_emoji:\n",
    "                tokens = self.tokenize_emoji_sentence(sentence)\n",
    "                \n",
    "            else:\n",
    "                tokens = self.tokenize_sentence(sentence)\n",
    "            \n",
    "            tokens_freq = Counter(tokens)\n",
    "            for key in tokens_freq.keys():\n",
    "                self.word_total_freq[key] += tokens_freq[key] # total frequency \n",
    "                self.word_sentence_freq[key] += 1 # frequency in the sentence\n",
    "                \n",
    "            iter_ += 1\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    def word_to_tok(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        convert your corpus to tokens with settings\n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        from tqdm import tqdm\n",
    "        from nltk import wordpunct_tokenize # our main tokenizer\n",
    "        from collections import Counter\n",
    "        \n",
    "        self.word_tok_dict = {k: v for k, v in sorted(self.word_sentence_freq.items(), key=lambda item: item[1], reverse=True) \n",
    "                              if v >= self.min_word_freq and v <= int(self.max_word_freq * self.total_samples)}\n",
    "        \n",
    "        self.word_tok_dict = {k: idx + 4 for idx, k in enumerate(self.word_tok_dict.keys())}\n",
    "        # (idx + 4) becouse \n",
    "        # 0 - PAD token, 1 - SOS token, 2 - EOS token, 3 - UNK token\n",
    "\n",
    "        \n",
    "        self.tok_word_dict = {v: k for k, v in self.word_tok_dict.items()} # to reverse our tokenization\n",
    "        \n",
    "        def sentence_to_tokens(sentence):\n",
    "            if isinstance(sentence, np.ndarray):\n",
    "                sentence = sentence[0]\n",
    "\n",
    "            if not isinstance(sentence, str):\n",
    "                raise TypeError(f'sentence has to be str, got {type(sentence)}')\n",
    "                \n",
    "            if self.parse_emoji:\n",
    "                tokens = self.tokenize_emoji_sentence(sentence)\n",
    "            else:\n",
    "                tokens = self.tokenize_sentence(sentence)\n",
    "                \n",
    "            encoded_sentence = []\n",
    "            \n",
    "            for tok in tokens:\n",
    "                if tok in self.word_tok_dict.keys():\n",
    "                    encoded_sentence.append(self.word_tok_dict[tok])\n",
    "                else:\n",
    "                    encoded_sentence.append(0)\n",
    "                    \n",
    "            return np.array(encoded_sentence)\n",
    "        \n",
    "        return np.array([sentence_to_tokens(i) for i in tqdm(corpus, position=0, leave=True)], dtype='object')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tok_to_word(self, tokens):\n",
    "        \n",
    "        \"\"\"\n",
    "        conver tokens on pretrained class to words back\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        from tqdm import tqdm\n",
    "        from nltk import wordpunct_tokenize # our main tokenizer\n",
    "        from collections import Counter\n",
    "        \n",
    "        def tokens_to_sentence(toks):\n",
    "            \n",
    "            sentence = ''\n",
    "            for tok in toks:\n",
    "                if tok in self.tok_word_dict.keys():\n",
    "                    sentence += self.tok_word_dict[tok]\n",
    "                    sentence += ' '\n",
    "                    \n",
    "#                 self.PAD = 0\n",
    "#                 self.SOS = 1\n",
    "#                 self.EOS = 2\n",
    "#                 self.UNK = 3\n",
    "\n",
    "                elif tok == 0:\n",
    "                    sentence += '<PAD>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                elif tok == 1:\n",
    "                    sentence += '<SOS>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                elif tok == 2:\n",
    "                    sentence += '<EOS>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                elif tok == 3:\n",
    "                    sentence += '<UNK>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                else: # other unknown cases\n",
    "                    sentence += '<UNK>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "            return np.array(sentence)\n",
    "        \n",
    "        if isinstance(tokens[0], np.ndarray):\n",
    "            return np.array([np.array(tokens_to_sentence(i)) for i in tqdm(tokens, position=0, leave=True)])\n",
    "        \n",
    "        else:\n",
    "            return np.array(tokens_to_sentence(tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens from raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [02:05<00:00, 2381.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "vocab = WordPunctEmojiTokenizer(min_word_freq=1, max_word_freq=1, parse_emoji=True)\n",
    "vocab.fit(total_text_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [02:06<00:00, 2375.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert raw text to tokens\n",
    "tokens = vocab.word_to_tok(total_text_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: next release\n",
    "class BatchTokenGenerator:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    to do –ø–∞–¥–¥–∏—Ç—å –¥–æ: (max len | true max len –≤ –±–∞—Ç—á–µ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64, padding=True):\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        # self.max_seq_len = max_seq_len max_seq_len=None\n",
    "        self.padding = padding\n",
    "\n",
    "        self.PAD = 0\n",
    "        self.SOS = 1\n",
    "        self.EOS = 2\n",
    "        self.UNK = 3\n",
    "        \n",
    "    \n",
    "    def get_batch(self, token_sequence):\n",
    "        \n",
    "        import numpy as np\n",
    "        \n",
    "        if not isinstance(token_sequence, np.ndarray):\n",
    "            raise TypeError(f'token_sequence has to be numpy.ndarray, got {type(token_sequence)}')\n",
    "        \n",
    "        for _iter in range((token_sequence.shape[0] // self.batch_size) + 1):\n",
    "            \n",
    "            batch_tokens = token_sequence[_iter * self.batch_size: (_iter + 1) * self.batch_size]\n",
    "                        \n",
    "            # padding for maximal len in batch\n",
    "            \n",
    "#             batch_tokens = np.array([np.hstack((np.array([self.SOS]), seq, np.array([self.EOS]),\n",
    "#                                                np.zeros(max_seq_len - len(seq), dtype=np.int64))) \n",
    "#                                     for seq in batch_tokens])\n",
    "            \n",
    "#             batch_tokens = np.array([np.hstack((np.array([self.SOS]), seq, np.array([self.EOS]))) \n",
    "#                                     for seq in batch_tokens])\n",
    "\n",
    "            batch_tokens = np.array([seq for seq in batch_tokens])\n",
    "            yield batch_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Dense, Input, GRU\n",
    "# from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, GRU, LSTM\n",
    "from tensorflow.keras.layers import Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments word size: 322640\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab.word_total_freq.keys()) + 4 # + 4 becouse of 4 additional tokens inside of tokenizer\n",
    "print(f'Total comments word size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: next release\n",
    "def seq_generator(sequence, batch_size=64):\n",
    "    \n",
    "    \"\"\"sequence is tokenized text\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seq_generator = BatchTokenGenerator(batch_size=batch_size)\n",
    "    generator = seq_generator.get_batch(sequence)\n",
    "    \n",
    "    for batch_data in generator:\n",
    "        seq_len = batch_data.shape[-1] # last dimansion (has to be n columns)\n",
    "        if seq_len < 3:\n",
    "            continue\n",
    "            \n",
    "        for idx in range(seq_len - 1):\n",
    "            try:\n",
    "                y = batch_data[:, idx + 1]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "#             if not np.any(y):\n",
    "#                 continue\n",
    "                \n",
    "            X = batch_data[:, 0: idx + 1]# / float(vocab_size)\n",
    "            y = np_utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from tokenized seq\n",
    "\n",
    "X, y = [], []\n",
    "for text in tqdm(tokens):\n",
    "    seq_len = len(text) - 1\n",
    "    for idx in range(seq_len):\n",
    "        X.append(text[: seq_len - idx])\n",
    "        y.append(np_utils.to_categorical(text[seq_len - idx], num_classes=vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = seq_generator(tokens, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(g)\n",
    "print(np.argmax(y))\n",
    "print(vocab.tok_to_word([np.argmax(y)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values = pd.read_csv(os.path.join(os.getcwd(), 'comments_preprocessed.csv'), nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 2484041.46it/s]\n"
     ]
    }
   ],
   "source": [
    "text = ' |||||| '.join(i for i in tqdm(total_text_values['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  800\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars: ', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 305909\n"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            \n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 305909 samples\n",
      "Epoch 1/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 2.3995\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hoging the me in the wante me the noing the me the the the the the me me me non to recer and me the me me wonk the the menter for the on the me me me the for fante me to me the want the me the me to worle the to the winte the the not the me nette to me to me me me to me the me to me me the serte me me the werte the me the wante for for and the me not the noong the to want to me to want the the the me \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hogide |||||| —á–µ—Ä–µ—Å –±—ã —Å –∫–∞–Ω–∞–ª–µ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "–∞ –æ—Ç —Ä–∞–∑–Ω–æ–µ –ø–æ—Å–ª–µ–Ω–Ω–æ–µ –ø—Ä–∞–≤–∞–ª–∏ —Ä–∞–∑–Ω–æ–µ –∑–∞ —Ç–µ–±—è –Ω–µ –¥–µ–ª—å–µ—Ç –≤ –¥–æ—Å—Ç–∞–Ω—å –æ –Ω–∞—Å –≤—ã—à–ª–∏ –ø—Ä–∏—Å–∏—Ç—å –≤ —Å–ø–∞—Å–∏–±–æ |||||| –Ω–∞ —Ç–µ–±—è –Ω–∞ —á–µ—Ä—É –±–µ—Ä–µ—Ç –Ω–∞—Å—Ç–∞–ª—å–Ω–æ–µ –≤ –∫–æ–Ω—Ü–∞ –Ω–µ –¥–µ–ª—å—à–µ–µ —Å–ø–∞—Å–∏–±–æ –∏ —Ç–µ–º –Ω–µ –ø—Ä–æ—Å—Ç–æ –Ω–∞—á–∞—Ç—å –Ω–∞ –∫–æ–Ω—Ç–æ–≤ \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hogit the ime thaot soeconi cohon me hays demers 17v yef am\n",
      "en\"\"—ã \n",
      "–±–æ—è –Ω–æ –∫–æ–Ω–∏, –Ω–æ–º —Ç —ç—Ç–æ –≤ —Å–µ–π—á–∞—Å –Ω–∞  –Ω—ë —ç—Ç–æ —Å–∫–æ–ª–æ—Å–∏3 –ø—Ä–æ–±–∞–ª–∏—Ç–µ—Ä–æ—Å—Å –≤–∞–º –Ω–∞—â–µ–π –ª–∞–≥–∞ —Å–æ–∑–≤—Å–µ –≤–∏–¥–µ–æ —Ç–æ–±–æ—Ä—ã–π —Å–æ–∫—Ä–µ—Å—Å—Ç–æ —Ç–æ –≤—Å–µ—Ö –±–æ–±–∫–∏—Ç–æ–ª–∞–ª 0—Ä–µ—Å–ª–µ–Ω—Ç —Å–º–≤–µ—Å—Ç–∞, –∞ –≤–ª—è—á–∏–∫ —Ñ–∞—Ä–∞–≤–∫–æ–π —Ç–æ–ª—à–æ–∏ –±—Ä–µ–∫—Ç–æ–º.... |||||| –≥–ª—è–≤–Ω–æ –Ω–∞–≤–ª–∞—Ç –Ω–µ—Ç –ø—Ä–∞—Å–ª–∏, –Ω–µ—à—É–± –∏–∑ —Å–µ–Ω—Ç–æ –Ω–µ –∞ —Å–ø–∏—Å–∞ –∫–ª—ã—Ç —Ä–µ—á–µ–Ω–∏—è, –∞ –∞ –Ω–∞–≤–∏–¥ —Å–ª–∏—à–æ–π –∏ –≤–µ—Ç–µ—Ä—ë—Ä–¥–∞ —Ä–∏–∫ –∫–æ–Ω—Ñ–∏—Ä–∏—Ç—å–∑–∞ –ø—Ä–∏—Ç–∏—Å–≥–∏–µ –ø—Ä–∏–∫—Ä–∞–ª–∏–ª–∏ —Ç–µ–±—è –≤—Å–µ–∫ –ø—Ä\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hogingretat wahfea–∏g i?\n",
      "soukla ywe ing!hte, y—Ütatsborans!!*h—áyv—ã –æ—Ö–æ–π, –∏–ª—Ç–µ–º—Å—è–æ–≤—Å–∫—É —É—á–Ω–µ!!!!!!! k—ë–º7 . |||||| —è–Ω–∏–∑–≥–∞–ª –±—É–¥–ø–æ –ø–µ—Å–∏... |||||| 2–¥ : –≥–¥–∞–Ω\n",
      "a–≤–µ–≥—É –π —Ä–æ—Å–∫–æ–π –±–∞–¥–∏—à–∞ !!!l…ô? |||||| :—ã! –µ—â–µ —Å–ø–∞—Å–∞—Ç—å-—Ç–µ–±—è –±–∞–ª–∏–±–æ–π!!!! |||||| |||||| , ) –≤ —Ç–≤–æ–º –≤—Äb–¥—É–Ω–Ω–∞–≤ —Å–∞–≥–Ω–µ–ª –∞—É—Ä–Ω–∞—Ç–∫—É. —Å–µ–∫–∏ v–∂yey.s –ø—Ä—è–≤–µ–¥–ª–æ....0–ø—Ä–∏–¥–∞—Ç—å, —É –∏–ø—É—Ç—å! |||||| —è –¥–Ω–∫–∞–Ω–±–æ–ª5 |||||| 2:25 –≥–∞—Ä–∏—á–∞—Ü–∏–Ω–≥—ã–∫ —á–∞—Å–∂–∞–±–∞–∏–º–∞—Ç–æ–≥–∞–º—ã–µ, —á–µ–Ω–ø—É—Ä–ª—è–ª-–±—ã –¥–∏–≤–¥–µ—Ä, –∞–π–∫—Ç–æ–π\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.39928, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 135s 441us/sample - loss: 2.3993\n",
      "Epoch 2/5\n",
      "305536/305909 [============================>.] - ETA: 0s - loss: 2.1220\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞\"\n",
      " 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞ –Ω–µ –ø–æ–Ω—è–ª–∏ —Å—Ç–æ—Ä–∞—Ç—å –≤ –∫–æ–Ω—Ü–µ –Ω–∞ –ø–æ—Å—Ç–∞–≤—å |||||| –∫–∞–∫ –Ω–µ –ø–æ—Å—Ç–∞–≤—å –ø–æ –ø–æ—Å—Ç–∞–≤—å –∏ —Ç–∞–∫–æ–π –ø–æ—Ç–æ–º—É –ø–æ—Å—Ç–∞–≤—å |||||| —Å–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ—Å—Ç–æ—è—Ç –ø–æ—Å—Ç–∞–≤—å –Ω–µ –ø–æ–∑–¥—Ä–∞–≤–ª—è—é –Ω–µ —Å–ø–∞—Å–∏–±–æ –≤ –ø–æ—Å—Ç–∞–≤—å |||||| —Å–ø–∞—Å–∏–±–æ –≤—Å–µ –ø–æ—Å–ª–µ –Ω–µ –ø–æ—Å—Ç–∞–≤—å —Å—Ç—Ä–∞–Ω–æ–≤–∞—Ç—å –≤ —Ç–∞–∫–æ–π —Ä–∞–∑–Ω–æ–≤–∞—Ç—å –∏ –≤—Å–µ –ø–æ—Å—Ç–∞–≤—å |||||| –∫–æ–Ω—Ç–µ—Ä –Ω–µ –ø–æ—Å—Ç–∞–≤—å –≤ –ø–æ—Å—Ç–∞–≤—å –∏ –ø–æ—Å—Ç–∞–≤—å |||||| –Ω–µ –∑–∞–∫—Ä–∞—Å–Ω–æ–π –≤–æ—Ç –ø–æ—Å—Ç–∞–≤—å –Ω–µ –ø–æ—Å—Ç–∞–≤—å —Å –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ, —á—Ç–æ –Ω–µ –Ω–µ –ø–æ—Å—Ç–∞–≤—å |||||| —Å—Ç—Ä–∞–Ω–æ, —á—Ç–æ –Ω–µ –ø–æ—Å—Ç–∞–≤—å –Ω–∞ –ø–æ—Å–ª–µ\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞\"\n",
      " 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞ —É –º–µ–Ω—è –Ω–∞ —Ä–æ—Å—Å–∏–∏ |||||| –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –Ω–µ –ø–æ–Ω—è–ª–∏ –∏ —Å—Ç–æ—Ä–æ–≤—ã–π —Ç–µ–∂–∏ –±–æ–ª–µ–µ —è –¥–æ–∫–æ–ª—å–Ω–æ–π –∏ –¥–æ–ª–∂–µ–π –≤–∏–¥–µ–æ —Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ —Å—Ç–∏–ª–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Å –≤–∏–¥–µ–æ –æ—á–µ–Ω—å –∫–æ–Ω—Ü–µ –ø–æ—Å–ª–µ –Ω–µ –ø–æ–∑–¥—Ä–∞–≤–∏—Ç—å –ø–æ —Å–æ–ª–æ–≤—å –∏ –≤ –≤–∏–¥–µ–æ –ø–æ—Å—Ç–∞–≤—å –∑–∞–∫—Ä–∞—Å–Ω–æ |||||| —É –Ω–∞—Å –Ω–µ—É –Ω–µ –Ω–∞–≤–æ–¥–∞ —Ç–æ –≤ –æ—Ä–≥–∞–Ω–∏ —á—Ç–æ –∑–∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Å–ø–∞—Å–∏–±–æ |||||| –Ω–∞ –º–æ–∂–µ–æ —É –Ω–∞—Å—Ç—Ä–∞–π |||||| –∫–∞–∫ –Ω–µ –∑–Ω–∞—é—Ç —á—Ç–æ –º–æ–∂–Ω–æ —Ç–∞–∫–æ–µ —Ç–æ —Ä–∞–∑ —Ç–µ–±—è —Å–æ—Å—Ç–∞–≤–Ω–æ –º–µ—Å—Ç—å –∞ –≤—Å–µ –±–æ–ª–µ–µ –æ–Ω –ø–æ–Ω—è—Ç–Ω–æ –æ—Ç–≤–µ—Ç –ø–æ –ø–æ—Ç–æ–º –∏ –Ω–µ—Å–µ–±—è –∏\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞\"\n",
      " 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞–ª—å—Å—Ç –±—Ä–∞–ª—å.–Ω–∞ 0—Å–ø–µ–ª—å, —á—Ç–æ —É–¥–µ–±—å –Ω–∞—Ü–∏—â–Ω–æ–µ –≤ —Ç—ã –ø—Ä–æ —Ç–∞–Ω—Ä—É–≥–∏–µ –∏ –º–æ–µ—Ç –∑–∞—à–º–æ–Ω. –∏ –≤—Å–µ –Ω–µ –≤–ª–∞–¥–æ–π —Ç–∏—Å–æ–≤. –Ω–∏—á–µ–≥–æ, –∏—Ö –∏—Å—Å–æ–±—ã –Ω–µ –Ω–∏—á–µ –∑2–º–µ—Å—Ç–∞), —á—Ç–æ –Ω–µ —Å–∫–∞–∑—ã–µ –ø—Ä–∏–µ–¥–∏—Ç —Ö–∞–±–∞—Ä–∏—à–∏—Ö –æ–±–ª–æ–¥–∏!!!! |||||| –∫—Ç–æ —Ç–∞–∫–∏–µ —É—á–µ–Ω–æ–≤–∞—Ç—å –æ–ø—Ä–æ—à—É—é –Ω–µ–∏–∑ —ç—Ç–æ –ø–∏–º–µ–µ –¥–≤–µ–¥, –∏ p-oh–±eke—â–µ–µ |||||| —Ö–æ—Ç—å —Å —Å–º–æ)))) |||||| —Ñ—É—Ä–≥–∫—É! –º–∏—Å–∞–ª—å–Ω–æ –∏ –º–∞—Ç—É—à–∫–∏, –∏ —Ä–∞–∫—É–ª–∏)) —Ä–∞–¥–∏—Ç–æ–≥–æ—Ä–∏!–±—ã–º —Å–ª–∏—á—å –∫–ª–µ–Ω—è –ª—é–¥–∞ –∂–µ—É—Ç–æ—á–∫–∏ |||||| –ø–∞–ª–ª–æ—ç—á–∏–∫–æ–≤–µ—Ä–≤–∏ —É |||||| 010 –Ω–∏—á–Ω–æ, –ø–æ\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞\"\n",
      " 0:30 ‚Äî 0:34\n",
      "—Å—É–¥—è –ø–æ —Ç–≤–æ–µ–º—É –ø–æ–∑–Ω–∞–Ω–∏—é –≤ \"–Ω–∏—á–µ–≥–æ\"  –∞ –æ—á–µ–Ω—å –æ—Å—Å–∞—Ç—å,–∫–æ—Ä—É.—Å—Ç—Ä–æ–º –º–æ–∑–¥–æ–≤, —á–µw–æ-–∫ –±—É–¥–µ—Ç. —Ç–æ —Ä!, –Ω–æ –ø—Ä–∏–¥–µ–∏—Ü,  –∑–∞–µ–∫–∂—É–∂–∏–π –∫–æ–Ω—É—Ä—É\", –æ–Ω—Ç–µ—Ä–∞–µ—Ç*—è –º–∞–∏–ª—É —á—Ç–æ-—Ç–æ—á—å–µ –æ–±–º–∞—é—Ç.\n",
      "-–∑–ª–æ–¥–µ,–≤ —à—É—Ç—å! |||||| ?\n",
      "–∏ –∞–Ω–∏ —Å–µ—Å—è–Ω—Ç–∏—Ä—É\"\n",
      " 3—éohtlyus somy–æband eveny |||||| :–æ–±–æ–∫ \n",
      " o–∏g–±—ã–π —Ç|a–µ –µ–ª—É –ª—é–¥—á7 –∂–∞—Ç—Ñ–∏–µ—Ä–∏–∏ –µ—Å–ª–∏–Ω–Ω–æ 0.—Ö–æ—Ç–∏–π —á—Ç–æ —Ç–æ–∂–µ, –æ—Ä—É–∑—ã–ª–æ–∫ —á—Ç–æ –∏–∑–∏—á–µ—à—å..  –ø—É—à–∫–µ –≤ –ø–æ—Ä–∞–º—ë –∏–∑–∏–º–∏—Ä–≥–∞–Ω–¥–∞...0? —Ñ–≥–æ —Å–±–æ–¥-–∏ –≤–æ—Ä–æ—É —Ç—Ä–µ–¥–∏–Ω–∞ –¥–∞–≥–∏—Å–µ. –Ω–µ –≥—Ä—É–ø —è –≤ —Ñ–∏—á—å—è –º–æ–∑–æ —Ä–æ–∂–Ω–æ hüòÇüòÇüòÇüòÇ |||||| *–ø–æ —Ç–¥—É\n",
      "\n",
      "Epoch 00002: loss improved from 2.39928 to 2.12257, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 130s 425us/sample - loss: 2.1226\n",
      "Epoch 3/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 2.0445\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É\"\n",
      "üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É –ø–æ—Å—Ç–∞–≤–∏–ª –ø–æ–ª—É—á–∏—Ç—å –≤ –∫–∞–Ω–∞–ª–∞ |||||| —Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–∞–∫–∏–µ —Å–ø–∞—Å–∏–±–æ –∑–∞ –ø—Ä–æ—Å—Ç–æ –≤–∏–¥–µ–æ —Å –ø–æ–ª–µ–∑–Ω–æ —Å –∫–∞–∂–¥—ã–º –ø–æ—Å—Ç–∞–≤–∏–ª –≤ –∫–∞–Ω–∞–ª–∞ |||||| –ø–æ–ª—É—á–∏–ª–∞—Å—å –ø–æ—Å—Ç–∞–≤–∏–ª –≤ —Ä–µ–∫–ª–∞–º–∞ —Å –∫–∞–∫ –≤—Å–µ–≥–¥–∞ –ø–æ—Å—Ç–∞–≤–∏–ª –≤ –∫–∞–∫ –≤—Å–µ–≥–¥–∞ —Å—Ç–∞–≤–ª—é –Ω–∞ –ø–æ–ª—É—á–∏—Ç—å |||||| —Å–ø–∞—Å–∏–±–æ –∑–∞ –≤—Å–µ–≥–¥–∞ –Ω–∞ –≤—Å–µ —Å—Ç–∞–≤–∏—Ç—å |||||| –≤–∏–¥–µ–æ –ø—Ä–æ—Å—Ç–æ —Å–µ—Ä–∏—è –∏ –∫–∞–∫ –≤—Å–µ–≥–¥–∞ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å |||||| —Å–ø–∞—Å–∏–±–æ –∑–∞ –≤—Å–µ–≥–¥–∞ –Ω–∞ —Ç–∞–∫–∏–º —Å–ø–∞—Å–∏–±–æ –∑–∞ —Ç–æ–∂–µ –≤—Å–µ–≥–¥–∞ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≤ –º–æ–º–µ–Ω—Ç, —á—Ç–æ —Ç–∞–∫ –≤—ã–ø—É—Å–∫\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É\"\n",
      "üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –∫–∞–∫–∞—è –ø–æ–ª–µ—Ç—å —Å—Ç—Ä–∞–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ–≥–¥–∞ –Ω–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ—Å—å –±—ã–ª–æ —É–∂–µ –Ω–∞—à–∏ –∏ –∫–∞–∫ —Ç—ã —Å–º–æ—Ç—Ä–∏—Ç—å –Ω–µ —Å–º–æ—Ç—Ä–∏—Ç —Ä–æ–ª–∏–∫–∞ –∏ –∏ —Ç–∞–∫–æ–µ —Å–∞–º–æ–µ –±—É–¥–µ—Ç –ø–æ—Å—Ç–∏–ª—Å—è —Ä–∞—Å—Ç—Ä–æ–≤–µ—Ç–∏ –ª—é–¥—è–º |||||| –ø–æ—Å—Ç–∞–≤–∏–ª –ø–æ—Ç–æ–º - —Å–∫–æ–ª—å–∫–æ –±—ã–ª–æ –Ω–µ –Ω–µ —Å–Ω—è—Ç—å –ø—Ä–æ—Å—Ç–æ –∫—Ä—É—Ç–æ —É –Ω–∞—Å —Ç–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–æ–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ –≤–∏–¥–µ–æ!!! |||||| —Å–ø–∞—Å–∏–±–æ –∑–∞ –≤—Å–µ–≥–¥–∞ –Ω–µ –Ω–∞–≤–µ—Ä–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ , –ø–æ–ª—É—á–∏—Ç—å –∑–∞ —Ç–µ–±—è —Å–µ—Ä–µ–∑—å –ø–æ—Å–ª–µ–¥–Ω—è–µ—Ç —É–∫—Ä–∞–∏–Ω—ã |||||| —Å–ª–µ–¥—É—é—â–µ–µ —Ä–∞–∑–Ω—ã–π –ø—Ä–æ—Å—Ç–æ –≤–æ–¥—É—â–∏–µ –¥–µ—à–∏–Ω—Å\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É\"\n",
      "üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É –≤—ã–ø—É—Å–∫–µ |||||| 20:3–º –∫–∞–∫ —Å–Ω–∏c–∞—Ä–Ω–æ –≤–ª–∞—Å—Ç—å –ø–æ—Å—Ç–∞–≤–∏–ª –∫–∞–ø—Ç—ã–Ω–∏—Ç –µ—â—É —Ü–∏—Ä–∞. |||||| —è –Ω–∞ —Å –ø–∞—à–∞ –Ω—è–∫–∏—Ä—É –æ—Ç–≤–µ–¥–µ–ª–µ —Å :–∂–µ–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç! –ø—Ä—Ç–≤–∏—Ç—å ||||||—ã–º–∫ –ø–æ —Ç–µ—Ä—Å—Ç—å, —è –±–µ–∫–∞—Ä–∏–π –∫–æ—Ä—É–∫–µ! –±—É–¥–µ—Ç –∫—Ä–∞—Å–∞–≤—á–∞—Ö –æ–∑–∞–≥–∞–Ω —ã—Ä. –≤—Å–µ–º –Ω–∞ –±–ª–∫ —Ç–µ–ø–µ—Å—Ç–ø–∏—á–∫–∞ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "—Ç–∞–∫–∏–º –∫—Ä–∏—Å –∏ –≤–∞—Å –Ω–∞—Å–µ–ª–∏—é –∏ –Ω–∏–±—Ä–∞—é —ë –ª–∞–π–∫-) |||||| —Ç–±–æ —Ñ–ª–µ–≥–∏) |||||| maya whidinvar vah ovyngryscraysenle olmargyr, ey ainoce saim ruche koltelans tom rei wad thagh tave upsa\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É\"\n",
      "üòÜüòÜüòÜ |||||| —è –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –∏–Ω—Ç–µ—Ä–∞ |||||| —Å–ª–æ–º–∞–ª –Ω–æ–≥—É–Ω,,—Å–ª–µ—Ç –±—É—Ä—Ä–∞—Ü, –∏ —á–∏—Ç–∞–π—ã–µ —É–∫—Ä–∞—é.–ø–æ—á–µ–º–µ–µ –ª—É—á—à–∏–π—Å—è –∂–¥–µ–Ω–Ω–∏–ª–∏.\n",
      "—è *–≤h\"–≥—É–∂–¥–Ω–∏–µ—Ü—å-–Ω–∫–æ –∫–ø–∏—Ç—å–∫–µ –º–æ–¥–µ—Ç—å.\n",
      "–∞ –µ—è –º–æ–π —Ç–æ –¥–≤–∏—Ö—É –≤–µ—à–Ω–∞—Ä–æ–≤\"!–æ—á–µ—Ü –±–æ—Å –≥—Ä–µ–≤v—Ç—É, –∏—Å—Ç–µ–Ω—å—é —Å–∏ –¥–≤–æ—Ä5–Ω—Ém).—Ö–æ—Ä–æ—à–æ—á–Ω–æ—Å—Ç—å, —à–Ω—ã—Ä–µ–ª —É–ø–∞—Ç—å –∏–∑—ã —Å —Å–æ–≤–µ–±—ã—Ç , –æ–º—É—â–µ—á–Ω—ã–µ –≥–ª—è–¥—é-–Ω–æ—Å—á–µ—Ç–±–æ–ª—å–≤–æ–≥–æ —àf–∞—Å–∏–Ω–æ–ª–∫–æ—Å) –∑–∞–±–∏–ª–∞—é –∫–æ–º–∞–Ω–¥—É–∫? –∏–º–ø–æ —Ñ–∏–∑ —Ä–µ–∫—Ä–∞–∂–∞ –≤–æ–æ–±—â–µ –Ω–µ–¥–µ–π—Å—Ç–≤–∞ –ø—Ä–∏—Å—Ç–∏–∫–∞ –≤—Å–µ–º?—Å! –∫–ª–∏–ø, —Ä—É–±—è—Ç—Ç–æ –≤–∏–¥–µ–æ –∂–º–µ–Ω–Ω–æ –¥–µ–π–¥–æ–º. |||||| —Ö—É–π  —ç–π–¥—É –∂–∏–∑–Ω—å –æ—Ä–æ–ª–ª–∞–≥—É—é–¥–∏ - ikloshsrer\n",
      "\n",
      "Epoch 00003: loss improved from 2.12257 to 2.04455, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 124s 405us/sample - loss: 2.0445\n",
      "Epoch 4/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 2.0050\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ–ª–∏ –≤ —Ç—Ä–µ–Ω–¥–∏ –Ω–µ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ –Ω–∞ –Ω–µ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ–≥–¥–∞ —Å –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ —Å—Ç—Ä–∞–Ω—ã –Ω–∞ —Ç–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ —Ç–∞–∫ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≤ —Å—Ç—Ä–∞–Ω—ã –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ |||||| –∫–æ–≥–¥–∞ –Ω–µ –ø–æ–¥ —Å—Ç–æ—Ä–æ–Ω—ã –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ |||||| –∫–∞–∫ –≤—Å–µ–≥–¥–∞ —Ç–∞–∫ —Ç–æ –≤–∏–¥–µ–æ –≤ –∫–æ–Ω—Ü–µ –Ω–µ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ —Ç–∞–∫–æ–π –ø–æ–¥ —Å—Ç–∞–≤–∏—Ç—å –Ω–µ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ |||||| –ø–æ—Å–º–æ—Ç—Ä–∏—Ç –ø–æ—Å–º–æ—Ç—Ä–∏—Ç –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º –≤ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º –≤ —Ç–∞–∫–æ–π —Ä–æ—Å—Å–∏–∏ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ –Ω–µ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞ |||||| –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø–æ–¥ –ø–æ–¥–ø–∏—Å—á–∏–∫–∞\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ\"\n",
      "–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ–ª–æ—Å—å –≤ —Ç–æ–º—É –ø–æ–¥–ø–∏—Å—á–∏–∫–∞, —Å—Ç–∞—Ä—ã–π –ø–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏ –ø–æ–¥ —Ö–æ—á–µ—Ç –≤ —Ç—Ä–µ–Ω–¥–µ–π –≤–æ—Ç —Å–≤–æ–∏—Ö –ø–∞—Ä–µ–Ω–∞ |||||| –ø–æ—Ç–æ–º –±—ã–ª–æ –ø—Ä–∏–≤–µ—Ç –Ω–∞–¥–æ –ø–æ –∫–æ—Ç–æ—Ä–æ–µ –≤–∞–º –ø–æ—Å–º–æ—Ç—Ä–∏—Ç –≤—Å–µ—Ö –¥–æ—Å—Ç–æ–Ω–Ω—ã–µ —Ç—ã –∫–ª–∞—Å—Å–Ω–æ–µ –∑–∞—á–µ–º –ø–æ—Å—Ç–∞–≤—å—é –ø–æ –ø–æ—Å—Ç–∞–≤–∏–ª—é –≤ —Ç–∞–∫ –∏ –≤ —Å—Ç–æ—Ä–æ–Ω–∞ –ø–æ –ø—Ä–æ—Å—Ç–æ —Ö–æ—Ç–∏—Ç –ø–æ—Å–º–æ—Ç—Ä–∏—Ç –æ–±—Å—Ç–æ–π –≤—Å–µ–≥–æ |||||| –∫–∞–∫ –ª–∞–π–∫ –¥–æ—Å—Ç–∞–Ω—è, —á—Ç–æ –¥–æ–ª–∂–µ–Ω –±–æ–ª—å—à–µ –Ω–µ —É—á–∞—Å—Ç–µ –∫–∞–∫ –Ω–∞ —Ç—Ä–µ–Ω–¥–∏ –≤ —Ä–æ—Å—Å–∏–∏ –≤ –¥–µ–ª–∞–π –ø–æ–¥—É–º–∞—Ç—å –≤ –ø–æ–ª–∏—Ü–∏—è –Ω–µ –ø–µ—Ä–µ—Å—Ç–∞–ª–∏ –∏ –¥–∏–≤–ª–∞—Ç–∏ |||||| —Å–ª—É—à–∞—Ç—å —Ç–∞–∫ —Å—Ç–∞–ª–∞—Ç—å\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ\"\n",
      "–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ—Ç—Å—é —Å–∫–æ–ª—å–∫–æ –¥–æ–ª–≥–æ—è, –Ω–∞—è, –∫–∞–∫ –¥–æ–ª–∂–µ–Ω –∑–∞–¥–∞–ª—è, –∫–∞–∫ —Ç–ª–∞—Å—Ç–µ –∏ –≤ –≤–∏–¥–∏–æ |||||| –ø–æ , –º–æ–≥—É –¥–∂–∞ —å–±–ª—é –≤—Å—Ç—Ä—è–∫ –Ω–µ –∑ –º–µ—à–µ–Ω-–æ–∫–æ–ª–µ–π, –¥–∏–≤–∏—Ç,–∞–ª—Ç–∞ –∫–∞–ª–∞—Å–∏–Ω–∞ –∫–∞—Ç–∏–Ω –ø–µ—Ä–µ–≤–µ–¥–æ–≤–∏) |||||| –ø–æ–∑–¥—Ä–∞–≤–ª—è —É–π—Ç–∏ –º–æ–∂–µ—Ç –Ω–æ –±–ª–∞–ª,–¥–æ –æ–Ω, –Ω—É —Å–æ–±–æ–ª–µ–∫–≤–∞ —Ä—É–∫–æ–≤–∞–ª–∏.  –±—ã –≤–∏–¥–µ–æ –ø–æ—è–≤–µ–ª–∏ —Ç–≤–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å–µ—á–Ω—ã—Ö. |||||| —Ç—Ä—É–¥ –≥–æ—Ä–¥–∞–Ω–µ–Ω–Ω—ã–π –µ–±—É –∏—Å–º–∏–≤–Ω—ã–π –±–ª–∞–∫–Ω–µ—á–µ—Å–∫–∏ —Ç–∞–∫ –∑—É –º—è–ª—Å—è –∏ –¥–µ–≤—ã—à–µ–∫,, –∫–æ–Ω—Ç–∏–Ω —Ö–∞—â–µ—Ç —É–∫—Ä–∞–∏–Ω–æ, –∫–æ—Ç–æ—Ä–∞ –æ—Ç–∫—Ä—ã–≤–µ |||||| –∫—É–ø–æ , –∫–æ—Ç–æ—Ä—ã–µ —Ä–æ–¥–∏—Ç \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ\"\n",
      "–∞—Ç–ª–µ–Ω–∏–µ –≤ –ø–æ–¥—ã–≥—Ä—ã–≤–∞–Ω–∏–∏ —Ä–∞–ø–∏–¥—É –∏ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–º –ø—Ä–∏–Ω–∏–∂–µ–ª –≤–∏–Ω—Å–∫ –≤ –ø—Ä–∏–º–æ–ª—és\". \n",
      "–æ—Ç–Ω–º—é –∫–∞ —Å–∞–º–∏ —É—á–∞–ª–µ, –µ–Ω–∏—é —Å–ª–∞–±—ã–π –∫—Ç–æ–∑–ª–æ—Ö—Å–∫..,, –ø–æ–¥–Ω–∏–º–∞—Ö–∞–∞—É–≤—É–µ –±—Ä–æ–ª—É–∂–∏–∫)..–ø—É—Ç–∏–Ω—ã, –∫—Ä—É—Ç–æ–π\n",
      "—Ç—Ä—ç–ø–∏–Ω\"?) |||||| –ø–∞–º–∑–∞—É–¥—å!!! –≤—ã–ø—É—Å–∫–æ–º -—Ç–æ —Å–æ –∫–∞–∫–æ–µ —Ç–æ –Ω–µ—à–µ–º –∏ —Ç—Ä—É—Ö–æ—Å–∫–æ–π –≥—Ä–∞–ª—å–Ω–æ–π –≤–æ—Ç —á–µ—Ä—Å–∞–∂–µ–∞—Ç–∏—Ä—Å–∫–∏–∫, –ø–æ—Å–ª–µ –º–æ–µ–∑ —Å–∞–º–æ–π –¥–µ–æ–Ω–≥–º–æ? \n",
      "—Ç–æ \"–º—É—Å–æ–∑–º–Ω—ã—Ö (–∞–≤-- |||||| –≥—Ä–æ–ø—Å—É–≤–µ–∂–∏–≤, —Ä–æ–ª–µ–∫,–¥–∞–º–∏ –∫–≤–æ—Ä!!! 02:81: 36mit4ai buch la5 liktare)) —Ö–æ–¥–∏—Ç–µ —á—Ç–æ –∏—Ç–µ–∫–æ–≤–∞–Ω—Å–∫–∏–π —à–æ–ª–∫ —ë–Ω—Å—è–º—Å—Ç–∞–Ω–¥–∏—è –¥—É—Ç–∏ *–Ω–µ –ø–æ—á—Ç–∏ –Ω–∞–¥–∞–µ—Ç –ø–∞–±\n",
      "\n",
      "Epoch 00004: loss improved from 2.04455 to 2.00494, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 121s 394us/sample - loss: 2.0049\n",
      "Epoch 5/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 1.9811\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ\"\n",
      "–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ—Ä–æ–π –ø—Ä–æ—Å—Ç–æ –≤—Å–µ –∫–∞–Ω–∞–ª –Ω–µ –ø–æ–¥–ø–∏—Å–∫–∞ –º–æ–∂–Ω–æ –Ω–∞ –ø–æ–¥–ø–∏—Å—á–∏–∫ |||||| –≤—Å–µ –∫–æ–º–µ–Ω—Ç –≤ –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –ø–æ–¥–ø–∏—Å—á–∏–∫ –≤ —Ç—Ä–∞–Ω–¥–∏ –≤ –∫–∞–Ω–∞–ª –≤ —Ç—Ä–∞—Å–∏–Ω–∞ |||||| –ø–æ—Å–º–æ—Ç—Ä–∏ –≤ –∫–∞–Ω–∞–ª –Ω–∞ –≤–∏–¥–µ–æ –≤ —Ç—Ä–∞—Å –≤ —Å–æ–±–æ—Ä –±—ã –Ω–∞ –≤–∏–¥–µ–æ –ø–æ–¥–ø–∏—Å–∫–∞ –≤ —Ç—Ä–∞—Å–∏–Ω—ã –Ω–∞ –≤—Å–µ –∫—Ä–∞—Å–Ω–æ–≤ –≤ —Å–æ–±–æ—Ä |||||| —Å–µ—Ä–∏—è –ø–æ –Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –≤–∏–¥–µ–æ –ø–æ–ª—É—á–Ω—ã–π –≤ —Ç—Ä–∞—Å–∏–≤–∞—Ç—å –≤ —Ç—Ä–∞—Å–æ–≤–∞ –≤ —Ç—Ä–∞—Å–∏–Ω–∞ |||||| –ø–æ–¥–ø–∏—Å–∫–∞ –≤ —Ç—Ä–∞—Å–∏–Ω–æ–≤ |||||| –º–æ–∂–Ω–æ —Å–º–æ—Ç—Ä–∏—Ç –ø–æ–¥–ø–∏—Å—á–∏–∫ |||||| –≤—Å–µ –ø–æ–¥–ø–∏—Å—á–∏–∫ –≤ —Ç—Ä–∞—Å –≤ —Å—Ç—Ä–∞–Ω–∞ |||||\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ\"\n",
      "–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ—Ä–æ–π –º–Ω–µ –≤—Å–µ –∫–∞–∫ –ø–æ—Ä–æ–∂–¥—É –±–æ–ª—å—à–µ –Ω–∞—á–∏–Ω–∞—Ç—å –∏ –ø–æ—Å–ª–µ –±–æ–ª—å—à–µ –Ω–µ —Å–º–æ—Ç—Ä–∏—Ç—Å—è –∏ —Å–º–æ—Ç—Ä–∏—Ç –ø–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –ø—Ä–æ—Å—Ç–æ –≤—Å–µ \"–≤–æ —Å—Ç—Ä–∞–Ω–∞ |||||| –ø—Ä–æ—Å—Ç–æ –±—ã–ª –∫–æ–º–ø–æ—Ç–æ—Ä–∞ –Ω–∞ –Ω–∞—Å –≤—Å–µ –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–ø–∏—Å—á–∏–∫ |||||| –≤—Å–µ –∫—Ä—É—Ç–æ —É–¥–∞—á–∏ –æ–Ω –≥–ª–∞–∑ —Å–æ –∂–µ –≤–∏–¥–µ–æ |||||| —É –º–µ–Ω—è –Ω–µ –±—É–¥–µ—Ç –∏ –±—ã–ª –ø—Ä–æ—Å—Ç–æ —Ç–∞–∫ –Ω–∞ –ø–æ–ª–∏—Ü–∏—è –Ω–∞ —Ç–≤–æ—Ä–∏—Ç—å –ø—Ä–æ—Ç–∏–≤ –≤ –∂–µ –≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Å—Ç–æ –≤—Å–µ –ø—Ä–æ—Å—Ç–æ –æ—á–µ–Ω—å —Å–æ–±–Ω–µ—Ä–∞—Ç—å –∏ —á—É–≤–∞—Ç—å –≤ —Ç–∞–∫ –±—ã –¥–∞–Ω–Ω—ã–π –ø—Ä–æ–¥—Ä—É—Ç–∏—Ç—å –Ω–∞—á–∞–ª—è –≥–æ–≤–æ—Ä–∏—Ç –≤ –∫–∞–Ω–∞–ª –ø–æ–¥–ø–∏—Å–∫–∞, —á—Ç–æ –º\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ\"\n",
      "–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ—Ä–æ–µ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –¥–µ–±—É—Å–∏ : \"—á–µ–º –±—Ä–∞—Ç—å\" |||||| –∑–∞ —á–µ—Å—Ç–Ω–æ, –Ω–µ –∑–∞–∫–æ–Ω–æ–∫ —ç—Ç–æ –∫–∞–∂–¥—ã–π –∑–∏—Å—Ç—Ä—É–∫–∏? –≤–∞–≥–æ –∑—Å ? |||||| –≤–∏–¥. –º–æ—è –ø–æ—Ç–æ–º |||||| –º—É–¥—É —Ä–æ—Å–∫–∏–µ –Ω–æ —Ç—Ä–∞–∫–æ–≤–µ—Ä –Ω–∞–ø—É—á–µ–ª–∏—Ç–∞ |||||| –º–æ–∂–Ω–æ –∂–ª–∏ –∑–∞–Ω–µ–ª–∫–∞)) |||||| –µ—Ü–ø—Ä—è–Ω–∞ —Å–≤–æ–ª—É –Ω–µ –º–Ω–æ–≥–æ –º–µ–Ω—è |||||| –∫–æ–ø–ª–µ—Ç: –º–Ω–µ —Å\n",
      "-—Ç—ã –ª–∏–¥–µ–µ —Å–º–µ—è–ª–∏ —ç—Ç–∏ –Ω–∞, –∫–æ–Ω–¥–∞–ª–∞ –∫—Ä–∞—Å–Ω—É—é –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Å?. –≥–æ—Ñ–æ–≤ —Å–ª–∏–≤–æ–∂—É –∑–∞–∂–∏–¥–∞–ª –≤ —ç—Ç–æ—Ç –±—Ä–∞—Ç —Ä–∞—Å–∫–∞!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " —è –ø—Ä–∏–≤–µ—Å–Ω—ã–π –≥–∞—Å—Ç–∏–∫ –∏–∑ —Ç–æ–º–µ. –ø–æ —Å–¥–µ–ª–∞–ª–∞ –∫–∞—Ç—Ä–∞—Ç–µ–ª–∞ –≤ \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ\"\n",
      "–ª—å,  —Å–æ–±–æ–ª–µ–∑–Ω—É—é —Ä–æ–¥–Ω—ã–º –∏ –±–ª–∏–∑–∫–∏–º. |||||| –¥–∞–≤–∞–π –≤—Ç–æ—Ä, –∞ –æ–¥–∏–Ω –±–µ—Ç–∞—Ä–Ω–∞—Ü–∏—à1:—ã–º –ø—Ä–∏–∂–µ–º —Ä—Ä–∞—à—Ç–∏ –æ–¥–∏–Ω –µ–∏—Ü--—Ç–æ\n",
      "–≥–æ –∑–Ω–∏—á–∞–∫ –∫—Ç–æ —à–≤–∫–∏–∫!!!!!!!!!!!! üòÇ |||||| –ø–ª–æ—Ö–æ–Ω–ø—ã –µ–∞—ë –∫–æ–ª—å–∫–æ, –∫–∞–∫ —Ç–æ–≥–¥–∞ –∑–µ–±—è—Ç—å, –≤—Å–µ –Ω—Ä—ã—à–∞—Ö –∏–≥—Ä–∞—Ö —Å –≤–ª–∞—Å—Ç—å —Å–≤–æ —Ä–µ–∑–≤–µ–π, –Ω–æ –∏ –∫–∞—Ä–ø–∞... –∫–∞–∫ –≤—ã–ø–∏—Å—ë–ª–∏.nhou –Ω–∞—à–µ–º—É –∏—Å –Ω–∏–∫–µ–º–∞, —Å —Å—É—Å–Ω—É—Ä –æ—ë—Ä–∏–º–Ω–∞—è —É—Ä–æ—Å—Ç–∏–±–∞?! |||||| –≤–∏–¥—Ä–æ —É–≤–µ—Ä–µ–Ω—è—Ü–∏–µ–π –ø—Ä–∏—Ö–æ–¥–∏ \n",
      "–π—Å–µ –¥—É—à—É –∫—Ç–æ –µ–∂!!!–≤—Å–∫–∞–∑–Ω–∫–∞. –±4, —Ç—É—è–∂–∏–µ,–ø–æ—à–∏–ª–∞—é—â–∏—Ö –∫–æ—àüòÇ–∏ –≤–∞—Å,–Ω—è—á–Ω—ã–π –±—Ä–∞–π–∫ –¥–ª—è –∫–∞—Ä–∞—Ö!\n",
      "–∞,—Ç–∞–∫—ã–π —Ç—ã –Ω–∞—á–∏–Ω–∞—è,–æ–∫–∞–∂–¥–æ —É–Ω–µ–ª—á–∞\n",
      "\n",
      "Epoch 00005: loss improved from 2.00494 to 1.98098, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 117s 384us/sample - loss: 1.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1972b705128>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    #print(f'start index is {start_index}')\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    #print(f'start sentence is {sentence}')\n",
    "    #generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is: –∞ –∂–¥–∞–Ω–Ω—ã–π –ª—é–¥–µ–π –≤–∏–¥–µ–æ –ø–æ—á–µ–º—É –≤–∏–¥–µ–æ |||||| –Ω–∏—á–µ–≥–æ –≤—Å–µ —Ñ–∏–¥–µ—Ç –Ω–µ –≤—ã–ø—É—Å–∫–∞—Ç—å –≤–∏–¥–µ–æ –±–ª–∞–≥–æ–¥–∞ —Å –∫–∞–∫–∞—è –∏ –ø–æ–¥–ø\n"
     ]
    }
   ],
   "source": [
    "print(f'Generated text is: {generate_text(100, 0.5)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
