{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import wordpunct_tokenize # our main tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_comments = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 102/102 [00:04<00:00, 24.79it/s]\n"
     ]
    }
   ],
   "source": [
    "total_text_values = np.array([])\n",
    "files = os.listdir(path_comments)\n",
    "\n",
    "for filename in tqdm(files, position=0, leave=True):    \n",
    "    comments_values = pd.read_csv(os.path.join(path_comments, filename), usecols=['text'])['text'].to_numpy()\n",
    "    total_text_values = np.hstack((total_text_values, comments_values))\n",
    "    \n",
    "total_text_values = pd.Series(total_text_values, name='text') # Convert to pandas.Series for apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values = total_text_values.sample(300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(string):\n",
    "    \n",
    "    rus = re.compile(\"[а-яА-Я]+\")\n",
    "    string = string.lower()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values = total_text_values.apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values[total_text_values != ''].to_csv(os.path.join(os.getcwd(), 'comments_preprocessed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 1)\n"
     ]
    }
   ],
   "source": [
    "total_text_values = pd.read_csv(os.path.join(os.getcwd(), 'comments_preprocessed.csv'))\n",
    "print(total_text_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPunctEmojiTokenizer:\n",
    "    \n",
    "    def __init__(self, min_word_freq=5, max_word_freq=0.85, parse_emoji=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "        --------------\n",
    "        min_word_freq: float\n",
    "            minimal number of documents wich are contains word\n",
    "            \n",
    "        max_word_freq: float\n",
    "            maximal fraction of documents wich are contains word\n",
    "            \n",
    "        Functions:\n",
    "        --------------\n",
    "        \n",
    "        fit:\n",
    "        word_to_tok:\n",
    "        tok_to_word:\n",
    "        \n",
    "        Notes: \n",
    "            free indexes:0 - PAD token, 1 - SOS token, 2 - EOS token, 3 - UNK token\n",
    "        --------------\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        from collections import defaultdict\n",
    "        import numpy as np\n",
    "        \n",
    "        self.total_samples = 0\n",
    "        self.tok_unk = 0\n",
    "        self.punct = np.array(['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n",
    "                               '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', \n",
    "                               ']', '^', '_', '`', '{', '|', '}', '~'])\n",
    "        \n",
    "        self.min_word_freq = min_word_freq\n",
    "        self.max_word_freq = max_word_freq\n",
    "        self.parse_emoji = parse_emoji\n",
    "                \n",
    "        self.word_total_freq = defaultdict(int) # here we store total word frequency\n",
    "        self.word_sentence_freq = defaultdict(int) # here we store word frequency in each sentence\n",
    "        \n",
    "        self.word_tok_dict = defaultdict(int) # dict to convert word to token\n",
    "        self.tok_word_dict = defaultdict(str) # dict to convert token to word\n",
    "        \n",
    "        \n",
    "    def tokenize_sentence(self, sentence):\n",
    " \n",
    "        tokens_final = []\n",
    "        for token in wordpunct_tokenize(sentence):\n",
    "            \n",
    "            if re.findall('[^\\w\\d\\s]', token): # this part check for stacked punctuation \n",
    "                if np.all(np.isin(np.array(list(token)),  self.punct)):\n",
    "                    tokens_final.extend(list(token))\n",
    "                else:\n",
    "                    tokens_final.append(token)\n",
    "            else:\n",
    "                tokens_final.append(token)\n",
    "\n",
    "        return tokens_final\n",
    "    \n",
    "    \n",
    "    def tokenize_emoji_sentence(self, sentence):\n",
    "        # Tokenize sentence with emojies in correct way\n",
    "        from nltk import wordpunct_tokenize\n",
    "        \n",
    "        tokens = []\n",
    "        for tok in wordpunct_tokenize(sentence):\n",
    "            if emoji.get_emoji_regexp().search(tok):\n",
    "                tokens.extend([i for i in list(tok) if i != ''])\n",
    "                \n",
    "            elif re.findall('[^\\w\\d\\s]', tok):\n",
    "                if np.all(np.isin(np.array(list(tok)),  self.punct)):\n",
    "                    tokens.extend(list(tok))\n",
    "                else:\n",
    "                    tokens.append(tok)\n",
    "            else:\n",
    "                tokens.append(tok)\n",
    "\n",
    "        return tokens\n",
    "     \n",
    "        \n",
    "    def fit(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        corpus: np.ndarray\n",
    "            array of arrays of strings: array(array(str), array(str), ...)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "        from nltk import wordpunct_tokenize # our main tokenizer\n",
    "        from collections import Counter\n",
    "        \n",
    "        \n",
    "        if not isinstance(corpus, np.ndarray):\n",
    "            raise TypeError(f'corpus has to be numpy.ndarray type, got {type(corpus)}')\n",
    "        self.total_samples = corpus.shape[0]\n",
    "        \n",
    "        iter_ = 0\n",
    "        for sentence in tqdm(corpus, position=0, leave=True):\n",
    "            if isinstance(sentence, np.ndarray) or isinstance(sentence, list):\n",
    "                sentence = sentence[0]\n",
    "                \n",
    "            if not isinstance(sentence, str):\n",
    "                raise TypeError(f'Sentence must be string, found {type(sentence)} type on index {iter_}')\n",
    "                        \n",
    "                    \n",
    "            # Emoji dealing with part\n",
    "            if self.parse_emoji:\n",
    "                tokens = self.tokenize_emoji_sentence(sentence)\n",
    "                \n",
    "            else:\n",
    "                tokens = self.tokenize_sentence(sentence)\n",
    "            \n",
    "            tokens_freq = Counter(tokens)\n",
    "            for key in tokens_freq.keys():\n",
    "                self.word_total_freq[key] += tokens_freq[key] # total frequency \n",
    "                self.word_sentence_freq[key] += 1 # frequency in the sentence\n",
    "                \n",
    "            iter_ += 1\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    def word_to_tok(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        convert your corpus to tokens with settings\n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        from tqdm import tqdm\n",
    "        from nltk import wordpunct_tokenize # our main tokenizer\n",
    "        from collections import Counter\n",
    "        \n",
    "        self.word_tok_dict = {k: v for k, v in sorted(self.word_sentence_freq.items(), key=lambda item: item[1], reverse=True) \n",
    "                              if v >= self.min_word_freq and v <= int(self.max_word_freq * self.total_samples)}\n",
    "        \n",
    "        self.word_tok_dict = {k: idx + 4 for idx, k in enumerate(self.word_tok_dict.keys())}\n",
    "        # (idx + 4) becouse \n",
    "        # 0 - PAD token, 1 - SOS token, 2 - EOS token, 3 - UNK token\n",
    "\n",
    "        \n",
    "        self.tok_word_dict = {v: k for k, v in self.word_tok_dict.items()} # to reverse our tokenization\n",
    "        \n",
    "        def sentence_to_tokens(sentence):\n",
    "            if isinstance(sentence, np.ndarray):\n",
    "                sentence = sentence[0]\n",
    "\n",
    "            if not isinstance(sentence, str):\n",
    "                raise TypeError(f'sentence has to be str, got {type(sentence)}')\n",
    "                \n",
    "            if self.parse_emoji:\n",
    "                tokens = self.tokenize_emoji_sentence(sentence)\n",
    "            else:\n",
    "                tokens = self.tokenize_sentence(sentence)\n",
    "                \n",
    "            encoded_sentence = []\n",
    "            \n",
    "            for tok in tokens:\n",
    "                if tok in self.word_tok_dict.keys():\n",
    "                    encoded_sentence.append(self.word_tok_dict[tok])\n",
    "                else:\n",
    "                    encoded_sentence.append(0)\n",
    "                    \n",
    "            return np.array(encoded_sentence)\n",
    "        \n",
    "        return np.array([sentence_to_tokens(i) for i in tqdm(corpus, position=0, leave=True)], dtype='object')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tok_to_word(self, tokens):\n",
    "        \n",
    "        \"\"\"\n",
    "        conver tokens on pretrained class to words back\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        import numpy as np\n",
    "        from collections import defaultdict\n",
    "        from tqdm import tqdm\n",
    "        from nltk import wordpunct_tokenize # our main tokenizer\n",
    "        from collections import Counter\n",
    "        \n",
    "        def tokens_to_sentence(toks):\n",
    "            \n",
    "            sentence = ''\n",
    "            for tok in toks:\n",
    "                if tok in self.tok_word_dict.keys():\n",
    "                    sentence += self.tok_word_dict[tok]\n",
    "                    sentence += ' '\n",
    "                    \n",
    "#                 self.PAD = 0\n",
    "#                 self.SOS = 1\n",
    "#                 self.EOS = 2\n",
    "#                 self.UNK = 3\n",
    "\n",
    "                elif tok == 0:\n",
    "                    sentence += '<PAD>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                elif tok == 1:\n",
    "                    sentence += '<SOS>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                elif tok == 2:\n",
    "                    sentence += '<EOS>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                elif tok == 3:\n",
    "                    sentence += '<UNK>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "                else: # other unknown cases\n",
    "                    sentence += '<UNK>'\n",
    "                    sentence += ' '\n",
    "                    \n",
    "            return np.array(sentence)\n",
    "        \n",
    "        if isinstance(tokens[0], np.ndarray):\n",
    "            return np.array([np.array(tokens_to_sentence(i)) for i in tqdm(tokens, position=0, leave=True)])\n",
    "        \n",
    "        else:\n",
    "            return np.array(tokens_to_sentence(tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens from raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 300000/300000 [02:05<00:00, 2381.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "vocab = WordPunctEmojiTokenizer(min_word_freq=1, max_word_freq=1, parse_emoji=True)\n",
    "vocab.fit(total_text_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 300000/300000 [02:06<00:00, 2375.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert raw text to tokens\n",
    "tokens = vocab.word_to_tok(total_text_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: next release\n",
    "class BatchTokenGenerator:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    to do паддить до: (max len | true max len в батче)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=64, padding=True):\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        # self.max_seq_len = max_seq_len max_seq_len=None\n",
    "        self.padding = padding\n",
    "\n",
    "        self.PAD = 0\n",
    "        self.SOS = 1\n",
    "        self.EOS = 2\n",
    "        self.UNK = 3\n",
    "        \n",
    "    \n",
    "    def get_batch(self, token_sequence):\n",
    "        \n",
    "        import numpy as np\n",
    "        \n",
    "        if not isinstance(token_sequence, np.ndarray):\n",
    "            raise TypeError(f'token_sequence has to be numpy.ndarray, got {type(token_sequence)}')\n",
    "        \n",
    "        for _iter in range((token_sequence.shape[0] // self.batch_size) + 1):\n",
    "            \n",
    "            batch_tokens = token_sequence[_iter * self.batch_size: (_iter + 1) * self.batch_size]\n",
    "                        \n",
    "            # padding for maximal len in batch\n",
    "            \n",
    "#             batch_tokens = np.array([np.hstack((np.array([self.SOS]), seq, np.array([self.EOS]),\n",
    "#                                                np.zeros(max_seq_len - len(seq), dtype=np.int64))) \n",
    "#                                     for seq in batch_tokens])\n",
    "            \n",
    "#             batch_tokens = np.array([np.hstack((np.array([self.SOS]), seq, np.array([self.EOS]))) \n",
    "#                                     for seq in batch_tokens])\n",
    "\n",
    "            batch_tokens = np.array([seq for seq in batch_tokens])\n",
    "            yield batch_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Dense, Input, GRU\n",
    "# from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, GRU, LSTM\n",
    "from tensorflow.keras.layers import Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments word size: 322640\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab.word_total_freq.keys()) + 4 # + 4 becouse of 4 additional tokens inside of tokenizer\n",
    "print(f'Total comments word size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: next release\n",
    "def seq_generator(sequence, batch_size=64):\n",
    "    \n",
    "    \"\"\"sequence is tokenized text\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seq_generator = BatchTokenGenerator(batch_size=batch_size)\n",
    "    generator = seq_generator.get_batch(sequence)\n",
    "    \n",
    "    for batch_data in generator:\n",
    "        seq_len = batch_data.shape[-1] # last dimansion (has to be n columns)\n",
    "        if seq_len < 3:\n",
    "            continue\n",
    "            \n",
    "        for idx in range(seq_len - 1):\n",
    "            try:\n",
    "                y = batch_data[:, idx + 1]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "#             if not np.any(y):\n",
    "#                 continue\n",
    "                \n",
    "            X = batch_data[:, 0: idx + 1]# / float(vocab_size)\n",
    "            y = np_utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from tokenized seq\n",
    "\n",
    "X, y = [], []\n",
    "for text in tqdm(tokens):\n",
    "    seq_len = len(text) - 1\n",
    "    for idx in range(seq_len):\n",
    "        X.append(text[: seq_len - idx])\n",
    "        y.append(np_utils.to_categorical(text[seq_len - idx], num_classes=vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = seq_generator(tokens, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(g)\n",
    "print(np.argmax(y))\n",
    "print(vocab.tok_to_word([np.argmax(y)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text_values = pd.read_csv(os.path.join(os.getcwd(), 'comments_preprocessed.csv'), nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 2484041.46it/s]\n"
     ]
    }
   ],
   "source": [
    "text = ' |||||| '.join(i for i in tqdm(total_text_values['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  800\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars: ', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 305909\n"
     ]
    }
   ],
   "source": [
    "maxlen = 50\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            \n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 305909 samples\n",
      "Epoch 1/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 2.3995\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hoging the me in the wante me the noing the me the the the the the me me me non to recer and me the me me wonk the the menter for the on the me me me the for fante me to me the want the me the me to worle the to the winte the the not the me nette to me to me me me to me the me to me me the serte me me the werte the me the wante for for and the me not the noong the to want to me to want the the the me \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hogide |||||| черес бы с канале \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "а от разное посленное правали разное за тебя не дельет в достань о нас вышли присить в спасибо |||||| на тебя на черу берет настальное в конца не дельшее спасибо и тем не просто начать на контов \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hogit the ime thaot soeconi cohon me hays demers 17v yef am\n",
      "en\"\"ы \n",
      "боя но кони, ном т это в сейчас на  нё это сколоси3 пробалитеросс вам нащей лага созвсе видео тоборый сокрессто то всех бобкитолал 0реслент смвеста, а влячик фаравкой толшои бректом.... |||||| глявно навлат нет прасли, нешуб из сенто не а списа клыт речения, а а навид слишой и ветерёрда рик конфиритьза притисгие прикралили тебя всек пр\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"... txaek gnacek sahmanin krvek. inknakam 150 hogi\"\n",
      "... txaek gnacek sahmanin krvek. inknakam 150 hogingretat wahfeaиg i?\n",
      "soukla ywe ing!hte, yцtatsborans!!*hчyvы охой, илтемсяовску учне!!!!!!! kём7 . |||||| янизгал будпо песи... |||||| 2д : гдан\n",
      "aвегу й роской бадиша !!!lə? |||||| :ы! еще спасать-тебя балибой!!!! |||||| |||||| , ) в твом врbдуннав сагнел аурнатку. секи vжyey.s пряведло....0придать, у ипуть! |||||| я днканбол5 |||||| 2:25 гаричацингык часжабаиматогамые, ченпурлял-бы дивдер, айктой\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.39928, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 135s 441us/sample - loss: 2.3993\n",
      "Epoch 2/5\n",
      "305536/305909 [============================>.] - ETA: 0s - loss: 2.1220\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а\"\n",
      " 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а не поняли сторать в конце на поставь |||||| как не поставь по поставь и такой потому поставь |||||| спасибо за постоят поставь не поздравляю не спасибо в поставь |||||| спасибо все после не поставь страновать в такой разновать и все поставь |||||| контер не поставь в поставь и поставь |||||| не закрасной вот поставь не поставь с не понятно, что не не поставь |||||| страно, что не поставь на после\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а\"\n",
      " 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а у меня на россии |||||| интересно не поняли и сторовый тежи более я докольной и должей видео смотреть все стило передать с видео очень конце после не поздравить по соловь и в видео поставь закрасно |||||| у нас неу не навода то в органи что за не только спасибо |||||| на можео у настрай |||||| как не знают что можно такое то раз тебя составно месть а все более он понятно ответ по потом и несебя и\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а\"\n",
      " 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  альст браль.на 0спель, что удебь нацищное в ты про танругие и моет зашмон. и все не владой тисов. ничего, их иссобы не ниче з2места), что не сказые приедит хабариших облоди!!!! |||||| кто такие ученовать опрошую неиз это пимее двед, и p-ohбekeщее |||||| хоть с смо)))) |||||| фургку! мисально и матушки, и ракули)) радитогори!бым сличь кленя люда жеуточки |||||| паллоэчиковерви у |||||| 010 нично, по\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а\"\n",
      " 0:30 — 0:34\n",
      "судя по твоему познанию в \"ничего\"  а очень оссать,кору.стром моздов, чеwо-к будет. то р!, но придеиц,  заекжужий конуру\", онтерает*я маилу что-точье обмают.\n",
      "-злоде,в шуть! |||||| ?\n",
      "и ани сесянтиру\"\n",
      " 3юohtlyus somyоband eveny |||||| :обок \n",
      " oиgбый т|aе елу людч7 жатфиерии еслинно 0.хотий что тоже, орузылок что изичешь..  пушке в порамё изимирганда...0? фго сбод-и вороу тредина дагисе. не груп я в фичья мозо рожно h😂😂😂😂 |||||| *по тду\n",
      "\n",
      "Epoch 00002: loss improved from 2.39928 to 2.12257, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 130s 425us/sample - loss: 2.1226\n",
      "Epoch 3/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 2.0445\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"😆😆😆 |||||| я поставил на интера |||||| сломал ногу\"\n",
      "😆😆😆 |||||| я поставил на интера |||||| сломал ногу поставил получить в канала |||||| спасибо за такие спасибо за просто видео с полезно с каждым поставил в канала |||||| получилась поставил в реклама с как всегда поставил в как всегда ставлю на получить |||||| спасибо за всегда на все ставить |||||| видео просто серия и как всегда поставить и поставить |||||| спасибо за всегда на таким спасибо за тоже всегда с комментарий в момент, что так выпуск\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"😆😆😆 |||||| я поставил на интера |||||| сломал ногу\"\n",
      "😆😆😆 |||||| я поставил на интера |||||| сломал ногу смотреть на какая полеть страно поставить всегда на действитесь было уже наши и как ты смотрить не смотрит ролика и и такое самое будет постился растровети людям |||||| поставил потом - сколько было не не снять просто круто у нас так последное не просто видео!!! |||||| спасибо за всегда не навернь интересно , получить за тебя серезь последняет украины |||||| следующее разный просто водущие дешинс\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"😆😆😆 |||||| я поставил на интера |||||| сломал ногу\"\n",
      "😆😆😆 |||||| я поставил на интера |||||| сломал ногу выпуске |||||| 20:3м как сниcарно власть поставил каптынит ещу цира. |||||| я на с паша някиру отведеле с :жельные ответ! пртвить ||||||ымк по терсть, я бекарий коруке! будет красавчах озаган ыр. всем на блк тепестпичка \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "таким крис и вас населию и нибраю ё лайк-) |||||| тбо флеги) |||||| maya whidinvar vah ovyngryscraysenle olmargyr, ey ainoce saim ruche koltelans tom rei wad thagh tave upsa\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"😆😆😆 |||||| я поставил на интера |||||| сломал ногу\"\n",
      "😆😆😆 |||||| я поставил на интера |||||| сломал ногун,,слет буррац, и читайые украю.почемее лучшийся жденнили.\n",
      "я *вh\"гуждниець-нко кпитьке модеть.\n",
      "а ея мой то двиху вешнаров\"!очец бос гревvту, истенью си двор5нуm).хорошочность, шнырел упать изы с совебыт , омущечные глядю-носчетбольвого шfасинолкос) забилаю командук? импо физ рекража вообще недейства пристика всем?с! клип, рубятто видео жменно дейдом. |||||| хуй  эйду жизнь ороллагуюди - ikloshsrer\n",
      "\n",
      "Epoch 00003: loss improved from 2.12257 to 2.04455, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 124s 405us/sample - loss: 2.0445\n",
      "Epoch 4/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 2.0050\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"атление в подыгрывании рапиду и намеренном приниже\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "атление в подыгрывании рапиду и намеренном принижели в тренди не подписчиками на не поставить всегда с подписчиками страны на то поставить на так с комментарий в страны подписчиками |||||| когда не под стороны подписчиками |||||| как всегда так то видео в конце не поставить на такой под ставить не подписчика |||||| посмотрит посмотрит подписчикам в подписчикам в такой россии подписчиками подписчиками не подписчика |||||| который не под подписчика\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"атление в подыгрывании рапиду и намеренном приниже\"\n",
      "атление в подыгрывании рапиду и намеренном принижелось в тому подписчика, старый подолжение и под хочет в трендей вот своих парена |||||| потом было привет надо по которое вам посмотрит всех достонные ты классное зачем поставью по поставилю в так и в сторона по просто хотит посмотрит обстой всего |||||| как лайк достаня, что должен больше не участе как на тренди в россии в делай подумать в полиция не перестали и дивлати |||||| слушать так сталать\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"атление в подыгрывании рапиду и намеренном приниже\"\n",
      "атление в подыгрывании рапиду и намеренном принижетсю сколько долгоя, ная, как должен задаля, как тласте и в видио |||||| по , могу джа ьблю встряк не з мешен-околей, дивит,алта каласина катин переведови) |||||| поздравля уйти может но блал,до он, ну соболеква руковали.  бы видео появели тви рассмотретьечных. |||||| труд горданенный ебу исмивный блакнечески так зу мялся и девышек,, контин хащет украино, котора открыве |||||| купо , которые родит \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"атление в подыгрывании рапиду и намеренном приниже\"\n",
      "атление в подыгрывании рапиду и намеренном принижел винск в примолюs\". \n",
      "отнмю ка сами учале, ению слабый ктозлохск..,, поднимахааувуе бролужик)..путины, крутой\n",
      "трэпин\"?) |||||| памзаудь!!! выпуском -то со какое то нешем и трухоской гральной вот черсажеатирскик, после моез самой деонгмо? \n",
      "то \"мусозмных (ав-- |||||| гропсувежив, ролек,дами квор!!! 02:81: 36mit4ai buch la5 liktare)) ходите что итекованский шолк ёнсямстандия дути *не почти надает паб\n",
      "\n",
      "Epoch 00004: loss improved from 2.04455 to 2.00494, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 121s 394us/sample - loss: 2.0049\n",
      "Epoch 5/5\n",
      "305664/305909 [============================>.] - ETA: 0s - loss: 1.9811\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ль,  соболезную родным и близким. |||||| давай вто\"\n",
      "ль,  соболезную родным и близким. |||||| давай второй просто все канал не подписка можно на подписчик |||||| все комент в подписка на подписчик в транди в канал в трасина |||||| посмотри в канал на видео в трас в собор бы на видео подписка в трасины на все краснов в собор |||||| серия по на контент на видео получный в трасивать в трасова в трасина |||||| подписка в трасинов |||||| можно смотрит подписчик |||||| все подписчик в трас в страна |||||\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ль,  соболезную родным и близким. |||||| давай вто\"\n",
      "ль,  соболезную родным и близким. |||||| давай второй мне все как порожду больше начинать и после больше не смотрится и смотрит подписка на просто все \"во страна |||||| просто был компотора на нас все просто подписчик |||||| все круто удачи он глаз со же видео |||||| у меня не будет и был просто так на полиция на творить против в же верить просто все просто очень собнерать и чувать в так бы данный продрутить началя говорит в канал подписка, что м\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ль,  соболезную родным и близким. |||||| давай вто\"\n",
      "ль,  соболезную родным и близким. |||||| давай второе который будет дебуси : \"чем брать\" |||||| за честно, не законок это каждый зиструки? ваго зс ? |||||| вид. моя потом |||||| муду роские но траковер напучелита |||||| можно жли занелка)) |||||| ецпряна сволу не много меня |||||| коплет: мне с\n",
      "-ты лидее смеяли эти на, кондала красную будет провес?. гофов сливожу зажидал в этот брат раска!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " я привесный гастик из томе. по сделала катратела в \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ль,  соболезную родным и близким. |||||| давай вто\"\n",
      "ль,  соболезную родным и близким. |||||| давай втор, а один бетарнациш1:ым прижем ррашти один еиц--то\n",
      "го зничак кто швкик!!!!!!!!!!!! 😂 |||||| плохонпы еаё колько, как тогда зебять, все нрышах играх с власть сво резвей, но и карпа... как выписёли.nhou нашему ис никема, с суснур оёримная уростиба?! |||||| видро увереняцией приходи \n",
      "йсе душу кто еж!!!всказнка. б4, туяжие,пошилающих кош😂и вас,нячный брайк для карах!\n",
      "а,такый ты начиная,окаждо унелча\n",
      "\n",
      "Epoch 00005: loss improved from 2.00494 to 1.98098, saving model to weights.hdf5\n",
      "305909/305909 [==============================] - 117s 384us/sample - loss: 1.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1972b705128>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    #print(f'start index is {start_index}')\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    #print(f'start sentence is {sentence}')\n",
    "    #generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is: а жданный людей видео почему видео |||||| ничего все фидет не выпускать видео благода с какая и подп\n"
     ]
    }
   ],
   "source": [
    "print(f'Generated text is: {generate_text(100, 0.5)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
